---
title: "01: Introduction to Estimation"
subtitle: "Stat061-F23"
author: "Prof Amanda Luby"
callout-appearance: minimal
knitr:
    opts_chunk: 
      dev: "ragg_png"
      echo: false
      warning: false
      message: false
format:
  pdf:
    include-in-header: 
       - "../preamble.tex"
    toc: false
    number-sections: true
    colorlinks: true
    geometry:
      - top=1in
      - left=1in
      - right=1in
      - bottom=1in
      - heightrounded
    fontfamily: libertine
    fontsize: 11pt
---

```{r, echo = FALSE}
library(tidyverse)
```

The first topic we're going to cover in this class is *estimation*. That is, how to use observed sample data to estimate population parameters. A health-care study, for example, might want to estimate the proportion of people who have private health insurance and the mean annual cost for those who have it. Some studies assume a particular *parametric* family of probability distributions for a response variable and then estimate the parameters of that distribution in order to fit the distribution to the data.

This set of notes covers the basics of estimating a parameter by constructing an *estimator*, that yields a single number, called a *point estimate*.

**Motivating Example:** The 2018 General Social Survey asked "Do you believe there is a life after death?" For the 2,123 people interviewed, one point estimate for the *population* proportion of Americans who would respond yes is the sample proportion, which was 0.81.

\vspace{1in}

# Definitions and Notation

Before we get started, let's re-introduce ourselves to some key definitions from probability, add some new definitions, and introduce the notation that we'll use.

::: callout-note
## Parameter

\vspace{.5in}
:::

::: callout-note
## Estimator

\vspace{.5in}
:::

::: callout-note
## Estimate

\vspace{.5in}
:::

::: callout-note
## Probability Density Function (PDF)

\vspace{.5in}
:::

::: callout-note
## Probability Mass Function (PMF)

\vspace{.5in}
:::

::: callout-note
## Likelihood function

\vspace{.5in}
:::

**Example: Binomial Distribution**

\vspace{2in}

```{r, fig.width= 4, fig.height=3}
tibble(
  pi = seq(0, 1, by = .01),
  L = seq(0, 1, by = .01)
) %>%
  ggplot(aes(x = pi, y = L)) + 
  labs(
    y = "Likelihood",
    x = expression("Binomial Parameter"~pi)
  ) + 
  theme_bw(base_size = 12, base_family = "Roboto")
```

\pagebreak 

**Exercise:** Is the likelihood function a probability distribution? Why or why not?

\vspace{1in}

**Exercise:** Suppose a researcher is collecting $n$ measurements of a continuous random variable $Y$, that they believe has the pdf $f_y(y; \theta) = \frac{1}{\theta^2} ye^{-y/\theta}$, $0 < y < \infty$, $0 < \theta < \infty$. What is the likelihood function? 

\vspace{2in}

# Method of Maximum Likelihood

**Example:** Continuing the example from above, what value of $\theta$ would *maximize* $L(\theta)$?

\vspace{2in}

::: callout-note
## Method of Maximum Likelihood

\vspace{1.5in}
:::

**Exercise:** If five data points have been recorded ($Y_i = 9.2, 5.6, 18.4, 12.1, 10.7$), what would the MLE from the previous example be?

\vspace{2in}

## Finding the MLE when more than one parameter is unknown

If the pdf or pmf that we're using has two or more parameters, say $\theta_1$ and $\theta_2$, finding MLEs for the $\theta_i$'s requires the solution of a set of simultaenous equations. We would typically need to solve the following system:

$$\frac{\delta \ln L(\theta_1, \theta_2)}{\delta\theta_1} = 0$$ $$\frac{\delta \ln L(\theta_1, \theta_2)}{\delta\theta_2} = 0.$$ **Example:** Suppose a random sample of size $n$ is drawn from the two parameter normal pdf

$$f_y(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2}(\frac{y-\mu}{\sigma})^2}.$$ Find the MLE's $\hat{\mu}$ and $\hat{\sigma^2}$.

\vspace{5in}

# Method of Moments

A second procedure for finding estimators of parameters is the *method of moments*. This method is often more tractable than the method of maximum likelihood when the underlying probability distribution has multiple parameters.

::: callout-note
## Moment

\vspace{.5in}
:::

**Example:** Suppose we draw $n$ random variables from $f_y(y;\theta) = \theta y^{\theta-1}$, $0 < y < 1$. \vspace{2in}

::: callout-note
## The Method of Moments

\vspace{1.5in}
:::

The method of moments is also especially helpful when we're working with named distributions (or functions of named distributions). 

**Exercise:** Find the MoM estimators for $\mu$ and $\sigma^2$ for the two parameter normal pdf.

\vspace{3in}

Some helpful "tricks" from probability: 

1. Named distributions

\vspace{1in}

2. Properties of expected value and variance

\vspace{1in}

3. Law of the unconscious statistician (LOTUS) 

\vspace{1in}

4. Moment generating functions

\vspace{1in}

